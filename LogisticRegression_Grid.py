# -*- coding: utf-8 -*-
"""MLFinalProjLR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUxpbS_JkpaCOKpsLJSv8MytUnXWddis
"""

# Logistic regression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.sparse import data
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from preprocess_utils import pre_processing, fit_vectorizer, crop


from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# Read data
dataset = pd.read_csv("./fake_or_real_news.csv")
dataset['title_text'] = dataset['title'] + ' ' + dataset['text']
print(dataset.head(10))

dataset['cropped'] = dataset.apply(lambda row: crop(row['title_text']), axis=1)
print(dataset.head(10))

preprocessed_dataset = pre_processing(dataset['cropped'])
# X_data = fit_vectorizer(preprocessed_dataset, vec_type="tfidf", max_features = 1500)

cv = CountVectorizer(max_features = 1500)       # Convert a collection of text documents to a matrix of token counts (the occurences of tokens in each document)
preprocessed_dataset_vectorized = cv.fit_transform(preprocessed_dataset).toarray()      # Learn the vocabulary dictionary and return document-term matrix
X_data = preprocessing.normalize(preprocessed_dataset_vectorized, norm='l2', axis=1, copy=True, return_norm=False)

le = preprocessing.LabelEncoder()
le.fit(dataset['label'])
y_binary = le.transform(dataset['label'])

classfier = LogisticRegression(max_iter=10000)

penalty = ['l2']
C = [0.01, 0.1, 1, 10, 100]
solver = ['newton-cg','lbfgs','liblinear','sag','saga']

grid = dict(penalty=penalty,C=C,solver=solver)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=classfier, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0, verbose=3)

grid_result = grid_search.fit(X_data, y_binary)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
