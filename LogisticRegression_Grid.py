# -*- coding: utf-8 -*-
"""MLFinalProjLR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUxpbS_JkpaCOKpsLJSv8MytUnXWddis
"""

# Logistic regression 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.sparse import data
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

# Read data
dataset = pd.read_csv("C:/Users/mgkeb/Documents/jupyter_notebooks/final_project/fake_or_real_news.csv/fake_or_real_news.csv")
dataset['title_text'] = dataset['title'] + ' ' + dataset['text']
print(dataset.head(10))


# Binarize the dataset
cv = CountVectorizer(max_features = 1500)                           # Convert a collection of text documents to a matrix of token counts (the occurences of tokens in each document)
X_title = cv.fit_transform(dataset['title_text']).toarray()         # Learn the vocabulary dictionary and return document-term matrix
print(X_title)

X = preprocessing.normalize(X_title, norm='l2', axis=1, copy=True, return_norm=False)   # Scale input vecotrs individually to unit norm (vector length)

le = preprocessing.LabelEncoder() 
le.fit(dataset['label']) 
y_binary = le.transform(dataset['label'])

# Dataset split (0.7/0.3)
# X_train, X_test, y_train, y_test = train_test_split(X_title, y_binary, test_size = 0.3, random_state = 0)

# Train the logistic regression classifier
# classifier = LogisticRegression(penalty='l2', fit_intercept=True, C=1, max_iter=1000)
classfier = LogisticRegression(max_iter=10000)

penalty = ['l2']
C = [0.01, 0.1, 1, 10, 100]
solver = ['newton-cg','lbfgs','liblinear','sag','saga']

grid = dict(penalty=penalty,C=C,solver=solver)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

grid_search = GridSearchCV(estimator=classfier, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0, verbose=3)

# classifier.fit(X_train, y_train)
grid_result = grid_search.fit(X, y_binary)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# Using the classifier to predict and evaluate the classfier
# y_pred = classifier.predict(X_test)
# print(len(y_pred))
# cm = confusion_matrix(y_test, y_pred)
# TP = cm[1][1]
# TN = cm[0][0]
# FP = cm[1][0]
# FN = cm[0][1]

# Accuracy = (TP + TN) / (TP + TN + FP + FN) 
# Precision = TP / (TP + FP)
# Recall = TP / (TP + FN)
# F1_Score = 2 * Precision * Recall / (Precision + Recall)

# print(Accuracy, Precision, Recall, F1_Score)